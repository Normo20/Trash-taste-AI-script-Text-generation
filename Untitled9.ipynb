{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "FZnz6Xgt4ZkE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "g4oX4Rs54bX3"
      },
      "outputs": [],
      "source": [
        "path_to_file = \"scripts.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2lM65Y94d2_",
        "outputId": "1cb523a9-45d1-414a-bb87-223d3113b4b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 143660 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAzJl-Vi_L4i",
        "outputId": "785c9a4c-acd2-4855-d7af-9a89e00adeba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "how do you put on a hoodie like a boomer\n",
            "i don't know how you've managed to find\n",
            "a way to do it it's a [ __ ] hoodie\n",
            "you've worn a hoodie for chris oh my god\n",
            "chris in a hoodie just looks like that\n",
            "[ __ ] like hello fellow kids\n",
            "hello everyone we\n"
          ]
        }
      ],
      "source": [
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-90FvAz_RI_",
        "outputId": "307d7e24-8658-42d9-a02d-60b94afd91cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t5LTdEi_f9o",
        "outputId": "33f0d80f-e0f8-41f4-c95f-af85dc6cc388"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ddaDoabX_ieq"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M60p1h2m_mBl",
        "outputId": "9b93bda7-b9e8-42c1-b453-07d1ff8ff324"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[22, 23, 24, 25, 26, 27, 28], [45, 46, 47]]>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "aUO2rRC8_n3V"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8W1t0sC_q-2",
        "outputId": "419cc4b7-ab6c-48b7-e57e-6a760fe97005"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDYuXN9B_sur",
        "outputId": "5bfb845d-8fcd-445c-ee0d-8bce539e8b0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "sXrrt4Gz_vQE"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I2axtg9_xLH",
        "outputId": "5c5667e3-d661-418b-ef91-7df15c678327"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(143660,), dtype=int64, numpy=array([29, 36, 44, ..., 46, 36, 42], dtype=int64)>"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "DuBN1uvX_zAH"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFtOD9OZ_1Bs",
        "outputId": "9294e225-6ca0-47c1-da61-98d06d9b3fbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "h\n",
            "o\n",
            "w\n",
            " \n",
            "d\n",
            "o\n",
            " \n",
            "y\n",
            "o\n",
            "u\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "IwWzaFva_2vY"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBBcZTPx_59M",
        "outputId": "1ca13b2a-3eee-4689-a1e1-829bdd9e680e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'h' b'o' b'w' b' ' b'd' b'o' b' ' b'y' b'o' b'u' b' ' b'p' b'u' b't'\n",
            " b' ' b'o' b'n' b' ' b'a' b' ' b'h' b'o' b'o' b'd' b'i' b'e' b' ' b'l'\n",
            " b'i' b'k' b'e' b' ' b'a' b' ' b'b' b'o' b'o' b'm' b'e' b'r' b'\\r' b'\\n'\n",
            " b'i' b' ' b'd' b'o' b'n' b\"'\" b't' b' ' b'k' b'n' b'o' b'w' b' ' b'h'\n",
            " b'o' b'w' b' ' b'y' b'o' b'u' b\"'\" b'v' b'e' b' ' b'm' b'a' b'n' b'a'\n",
            " b'g' b'e' b'd' b' ' b't' b'o' b' ' b'f' b'i' b'n' b'd' b'\\r' b'\\n' b'a'\n",
            " b' ' b'w' b'a' b'y' b' ' b't' b'o' b' ' b'd' b'o' b' ' b'i' b't' b' '\n",
            " b'i' b't' b\"'\"], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSrLWlyB_791",
        "outputId": "88a58d98-c00f-449a-d372-381f758c88e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b\"how do you put on a hoodie like a boomer\\r\\ni don't know how you've managed to find\\r\\na way to do it it'\"\n",
            "b\"s a [ __ ] hoodie\\r\\nyou've worn a hoodie for chris oh my god\\r\\nchris in a hoodie just looks like that\\r\\n\"\n",
            "b'[ __ ] like hello fellow kids\\r\\nhello everyone welcome back to another\\r\\nepisode of the trash taste pod'\n",
            "b\"cast i'm\\r\\nyour host for today joey and i'm with\\r\\nthe boys as per usual and we have a\\r\\nfamiliar face\\r\\n\"\n",
            "b\"all right this is like the biggest\\r\\npodcast in the world\\r\\nthere's black people moving the set\\r\\naround\"\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "M2Zk7qPB_-Zt"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzbrSc_CAAiZ",
        "outputId": "ceea3d51-f562-4d65-f90c-6702560fe614"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "f5_BySQIADYQ"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJKU5Fh9AF5e",
        "outputId": "e6c49b36-8210-433d-abde-20b2139b8717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input : b\"how do you put on a hoodie like a boomer\\r\\ni don't know how you've managed to find\\r\\na way to do it it\"\n",
            "Target: b\"ow do you put on a hoodie like a boomer\\r\\ni don't know how you've managed to find\\r\\na way to do it it'\"\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P-XdnChAHsw",
        "outputId": "d89d13ad-5a9c-4338-f8d6-14370a509276"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "QdWXYDu_ANFj"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "9eAPj2oIAPvA"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else: \n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "xbZRkzjYASoV"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdBfQk9bAUa1",
        "outputId": "046a755f-717d-4558-ccda-852773062f20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100, 48) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TiLf1ByAXaj",
        "outputId": "1f79064d-0068-4a6a-a91a-080121f84fc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  12288     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  49200     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,999,792\n",
            "Trainable params: 3,999,792\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "wKDqlF-eAdxA"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4Ty0asgAgzf",
        "outputId": "7bed1e20-1043-4893-e6a5-215d7bae2030"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 2, 32, 13,  3, 45, 17, 32,  0, 37, 20,  3, 45,  8,  5, 39, 41,  9,\n",
              "       35, 44,  2, 41, 30, 46, 34, 43,  8, 35, 36, 24, 18, 40, 44, 30, 28,\n",
              "       42, 17, 18, 11, 19, 35,  4, 42, 25, 40, 28, 46, 27,  6, 27, 19,  2,\n",
              "       44, 28,  8, 16, 23,  7, 29, 25, 14, 27, 34, 20, 46, 34, 35,  5, 27,\n",
              "       20, 35, 43, 35, 19, 34,  6, 37,  5, 15, 13, 15, 41, 24, 21, 29, 37,\n",
              "       28, 32, 29, 21,  2,  0, 38, 34, 36, 23,  9, 23, 11,  3, 30],\n",
              "      dtype=int64)"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBdu7eiEAi3l",
        "outputId": "2b5932be-bcf9-4fd5-b406-5ee119958482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b\" insane how\\r\\nmuch they utilize they knew exactly what\\r\\nthey were doing and so much yeah it's\\r\\nlike a\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"\\rk6 xLk[UNK]p] x1-rt2nw\\rtiymv1nocMswiguLM4[n'udsgyf.f[\\rwg19b0hd7fm]ymn-f]nvn[m.p-868tc_hpgkh_\\r[UNK]qmob2b4 i\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "DW9i5TSvAk2j"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKVnPSz1AqMw",
        "outputId": "f4b292a0-9cce-49a8-c258-a156223caf32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 48)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         3.8713143\n"
          ]
        }
      ],
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm5d2eosAsGt",
        "outputId": "30d7a3bc-7a6b-4982-c8f3-a3e8f016f4a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "48.005436"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.exp(mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "9WIuYdAEAu6j"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "mIaYTbl8Aw7X"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './deneme2'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "0h5RaKaXAytt"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "id": "IK2NRMDlBCWO",
        "outputId": "4967aef0-5ccb-40c8-80ca-7ce3825c1824"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "22/22 [==============================] - 30s 1s/step - loss: 3.6968\n",
            "Epoch 2/50\n",
            "22/22 [==============================] - 30s 1s/step - loss: 2.8785\n",
            "Epoch 3/50\n",
            "22/22 [==============================] - 30s 1s/step - loss: 2.4850\n",
            "Epoch 4/50\n",
            "22/22 [==============================] - 30s 1s/step - loss: 2.2767\n",
            "Epoch 5/50\n",
            "22/22 [==============================] - 30s 1s/step - loss: 2.1679\n",
            "Epoch 6/50\n",
            "22/22 [==============================] - 30s 1s/step - loss: 2.0847\n",
            "Epoch 7/50\n",
            "22/22 [==============================] - 30s 1s/step - loss: 2.0042\n",
            "Epoch 8/50\n",
            "22/22 [==============================] - 31s 1s/step - loss: 1.9282\n",
            "Epoch 9/50\n",
            "22/22 [==============================] - 30s 1s/step - loss: 1.8582\n",
            "Epoch 10/50\n",
            "22/22 [==============================] - 31s 1s/step - loss: 1.7917\n",
            "Epoch 11/50\n",
            "22/22 [==============================] - 30s 1s/step - loss: 1.7312\n",
            "Epoch 12/50\n",
            "22/22 [==============================] - 30s 1s/step - loss: 1.6766\n",
            "Epoch 13/50\n",
            "22/22 [==============================] - 30s 1s/step - loss: 1.6240\n",
            "Epoch 14/50\n",
            "22/22 [==============================] - 30s 1s/step - loss: 1.5727\n",
            "Epoch 15/50\n",
            "22/22 [==============================] - 33s 1s/step - loss: 1.5221\n",
            "Epoch 16/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 1.4783\n",
            "Epoch 17/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 1.4380\n",
            "Epoch 18/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 1.3960\n",
            "Epoch 19/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 1.3565\n",
            "Epoch 20/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 1.3200\n",
            "Epoch 21/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 1.2819\n",
            "Epoch 22/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 1.2446\n",
            "Epoch 23/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 1.2073\n",
            "Epoch 24/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 1.1712\n",
            "Epoch 25/50\n",
            "22/22 [==============================] - 31s 1s/step - loss: 1.1327\n",
            "Epoch 26/50\n",
            "22/22 [==============================] - 31s 1s/step - loss: 1.0944\n",
            "Epoch 27/50\n",
            "22/22 [==============================] - 31s 1s/step - loss: 1.0542\n",
            "Epoch 28/50\n",
            "22/22 [==============================] - 31s 1s/step - loss: 1.0143\n",
            "Epoch 29/50\n",
            "22/22 [==============================] - 31s 1s/step - loss: 0.9715\n",
            "Epoch 30/50\n",
            "22/22 [==============================] - 31s 1s/step - loss: 0.9252\n",
            "Epoch 31/50\n",
            "22/22 [==============================] - 31s 1s/step - loss: 0.8766\n",
            "Epoch 32/50\n",
            "22/22 [==============================] - 31s 1s/step - loss: 0.8297\n",
            "Epoch 33/50\n",
            "22/22 [==============================] - 31s 1s/step - loss: 0.7769\n",
            "Epoch 34/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 0.7281\n",
            "Epoch 35/50\n",
            "22/22 [==============================] - 31s 1s/step - loss: 0.6732\n",
            "Epoch 36/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 0.6140\n",
            "Epoch 37/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 0.5577\n",
            "Epoch 38/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 0.5038\n",
            "Epoch 39/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 0.4500\n",
            "Epoch 40/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 0.3977\n",
            "Epoch 41/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 0.3472\n",
            "Epoch 42/50\n",
            "22/22 [==============================] - 33s 1s/step - loss: 0.3053\n",
            "Epoch 43/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 0.2676\n",
            "Epoch 44/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 0.2352\n",
            "Epoch 45/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 0.2056\n",
            "Epoch 46/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 0.1814\n",
            "Epoch 47/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 0.1590\n",
            "Epoch 48/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 0.1421\n",
            "Epoch 49/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 0.1271\n",
            "Epoch 50/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 0.1157\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "lloGkWymBGhl"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature=temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())]) \n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits] \n",
        "    predicted_logits, states =  self.model(inputs=input_ids, states=states, \n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "    \n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "fVYxatIOD7DO"
      },
      "outputs": [
        {
          "ename": "InvalidArgumentError",
          "evalue": "indices[1] = [0] is repeated [Op:SparseToDense]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "Input \u001b[1;32mIn [80]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m one_step_model \u001b[38;5;241m=\u001b[39m \u001b[43mOneStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchars_from_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids_from_chars\u001b[49m\u001b[43m)\u001b[49m\n",
            "Input \u001b[1;32mIn [79]\u001b[0m, in \u001b[0;36mOneStep.__init__\u001b[1;34m(self, model, chars_from_ids, ids_from_chars, temperature)\u001b[0m\n\u001b[0;32m     10\u001b[0m skip_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mids_from_chars([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m'\u001b[39m])[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     11\u001b[0m sparse_mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mSparseTensor(\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Put a -inf at each bad index.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(skip_ids),\n\u001b[0;32m     14\u001b[0m     indices \u001b[38;5;241m=\u001b[39m skip_ids,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Match the shape to the vocabulary\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     dense_shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mlen\u001b[39m(ids_from_chars\u001b[38;5;241m.\u001b[39mget_vocabulary())]) \n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse_mask\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mC:\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py:1711\u001b[0m, in \u001b[0;36msparse_tensor_to_dense\u001b[1;34m(sp_input, default_value, validate_indices, name)\u001b[0m\n\u001b[0;32m   1708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m default_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1709\u001b[0m   default_value \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mzeros([], dtype\u001b[38;5;241m=\u001b[39msp_input\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m-> 1711\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_sparse_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_to_dense\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43msp_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43msp_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43msp_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mC:\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_sparse_ops.py:3161\u001b[0m, in \u001b[0;36msparse_to_dense\u001b[1;34m(sparse_indices, output_shape, sparse_values, default_value, validate_indices, name)\u001b[0m\n\u001b[0;32m   3159\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 3161\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   3163\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "File \u001b[1;32mC:\\Python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7186\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7185\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7186\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
            "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[1] = [0] is repeated [Op:SparseToDense]"
          ]
        }
      ],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gvhEDNdD9LF",
        "outputId": "083bc4dc-1dfd-47ef-f87c-7b1dfbaee07e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'one_step_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[1;32mIn [81]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m result \u001b[38;5;241m=\u001b[39m [next_char]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m   next_char, states \u001b[38;5;241m=\u001b[39m \u001b[43mone_step_model\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate_one_step(next_char, states\u001b[38;5;241m=\u001b[39mstates)\n\u001b[0;32m      8\u001b[0m   result\u001b[38;5;241m.\u001b[39mappend(next_char)\n\u001b[0;32m     10\u001b[0m result \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mstrings\u001b[38;5;241m.\u001b[39mjoin(result)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'one_step_model' is not defined"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Ceza:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(10000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "\n",
        "print(f\"\\nRun time: {end - start}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4peugiDBEKru",
        "outputId": "4f1dd69f-b73e-469c-fe21-e7eab88b1e69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Feb  3 21:58:15 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 511.65       Driver Version: 511.65       CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ... WDDM  | 00000000:26:00.0  On |                  N/A |\n",
            "| 31%   38C    P5    24W / 175W |    558MiB /  8192MiB |     34%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1020    C+G                                   N/A      |\n",
            "|    0   N/A  N/A      1452    C+G                                   N/A      |\n",
            "|    0   N/A  N/A      2492    C+G   ....0.22\\OverwolfBrowser.exe    N/A      |\n",
            "|    0   N/A  N/A      6744    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
            "|    0   N/A  N/A      8208    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
            "|    0   N/A  N/A      8484    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A      8500    C+G   N:\\Overwolf\\Overwolf.exe        N/A      |\n",
            "|    0   N/A  N/A      8748    C+G   ...8bbwe\\Microsoft.Notes.exe    N/A      |\n",
            "|    0   N/A  N/A     10260    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
            "|    0   N/A  N/A     10868    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
            "|    0   N/A  N/A     11076    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
            "|    0   N/A  N/A     11244    C+G   ...er_engine\\wallpaper32.exe    N/A      |\n",
            "|    0   N/A  N/A     12108    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
            "|    0   N/A  N/A     13460    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n",
            "|    0   N/A  N/A     14128    C+G   ...obeNotificationClient.exe    N/A      |\n",
            "|    0   N/A  N/A     15084    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-6HYs3AEMSk",
        "outputId": "0b3b430d-8b2b-4e70-841e-9f663afcfaa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 146442 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.24-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.24-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.24-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsgV3cmNEqma",
        "outputId": "c987d254-c20f-417b-c422-5150ab46eda2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Feb  3 21:43:39 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 511.23       Driver Version: 511.23       CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ... WDDM  | 00000000:26:00.0  On |                  N/A |\n",
            "| 31%   42C    P8    22W / 175W |    845MiB /  8192MiB |      1%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1444    C+G                                   N/A      |\n",
            "|    0   N/A  N/A      3296    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
            "|    0   N/A  N/A      8264    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
            "|    0   N/A  N/A      9784    C+G   ...8bbwe\\Microsoft.Notes.exe    N/A      |\n",
            "|    0   N/A  N/A      9920    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A      9924    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
            "|    0   N/A  N/A     10244    C+G                                   N/A      |\n",
            "|    0   N/A  N/A     10668    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
            "|    0   N/A  N/A     10784    C+G   ...obeNotificationClient.exe    N/A      |\n",
            "|    0   N/A  N/A     10792    C+G   N:\\Overwolf\\Overwolf.exe        N/A      |\n",
            "|    0   N/A  N/A     11856    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
            "|    0   N/A  N/A     12080    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n",
            "|    0   N/A  N/A     13176    C+G   ...er_engine\\wallpaper32.exe    N/A      |\n",
            "|    0   N/A  N/A     13484    C+G   ....0.22\\OverwolfBrowser.exe    N/A      |\n",
            "|    0   N/A  N/A     15416    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
            "|    0   N/A  N/A     15500    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
            "|    0   N/A  N/A     16136    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
            "|    0   N/A  N/A     18324    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
